# DQN CartPole Baseline

A simple implementation of a Deep Q-Network (DQN) to solve the classic CartPole-v1 balancing problem using OpenAI Gym. This project serves as a baseline for understanding reinforcement learning (RL) concepts and the DQN algorithm.

ğŸš€ Overview
- Implements a DQN agent to balance a pole on a moving cart.
- Uses experience replay and a target network for stable training.
- Trains with an epsilon-greedy exploration strategy and decaying epsilon.
- Provides plots for rewards and loss trends during training and evaluation.
- Saves the trained model (cartpole_dqn_model.h5) for reuse.

âš™ï¸ Environment
- Environment: CartPole-v1
- Observation Space: 4 values (cart position, cart velocity, pole angle, pole angular velocity).
- Action Space: 2 discrete actions {0 = push left, 1 = push right}.
- Reward: +1 per time step pole remains balanced.
- Episode ends if:
  â€¢ Pole angle > Â±12Â°
  â€¢ Cart position > Â±2.4
  â€¢ Episode length > 500 steps

ğŸ“š Key Concepts
- Bellman Equation for Q-learning updates
- Experience Replay: Stores transitions for batch updates
- Target Network: Stabilizes Q-value estimation
- Epsilon Decay: Shifts from exploration â†’ exploitation

ğŸ”§ Implementation
- Frameworks: TensorFlow / Keras, NumPy, Pandas, Matplotlib
- Neural Network:
  â€¢ Input: 4 state features
  â€¢ Hidden layers: 2 Ã— Dense(128, ReLU)
  â€¢ Output: Q-values for each action

ğŸ“Š Results
- Trained over 700 episodes with batch size 32.
- Average reward increased steadily after ~400 episodes.
- Loss decreased over time with some fluctuations.
- Test runs achieved stable rewards >100 per episode.

ğŸ“‚ Files
- Final DQN.ipynb â†’ Main training notebook

â–¶ï¸ Run
```bash
# Install dependencies
pip install gym tensorflow numpy pandas matplotlib

# Run training
python Final_DQN.ipynb
```
ğŸ“Œ References
- Barto, Sutton, Anderson (1983) Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems
- Gym Docs: https://www.gymlibrary.dev/environments/classic_control/cart_pole/
